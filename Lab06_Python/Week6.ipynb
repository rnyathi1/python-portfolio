{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f2e54b-572f-43d5-89aa-fc323f322cdd",
   "metadata": {},
   "source": [
    "# Practical Business Analytics Week 6 Lab - Unsupervised Machine Learning\n",
    "\n",
    "The lab from this week looks at some unsupervised machine learning methods.\n",
    "\n",
    "Work through the cells in this Jupyter Notebook, following the instructions in the text boxes to load and analyse the data.\n",
    "\n",
    "Run the cell below to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703e107-a0c2-40a6-9f89-553c22d94238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18b778-d7ed-484a-a915-aea2db81e0ad",
   "metadata": {},
   "source": [
    "First we will read in the data from the file provided on SurreyLearn. Run this code in the cell below:\n",
    "\n",
    "```python\n",
    "data = pd.read_csv(\"UCI-G.csv\")\n",
    "data.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59c638-b5a7-4c63-a022-62f1ca449170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a61ef4-c0f0-4af9-b0d7-b51ac988c417",
   "metadata": {},
   "source": [
    "In this data you can see that in some columns, there are features that are ordinal, for example in the savings column. To see this, we can list the unique values in the savings column using\n",
    "\n",
    "```python\n",
    "data['Savings'].unique()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72406ab-b523-4576-b319-7e273b0cabaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5072f6ec-0ef9-4fb4-963f-9d3e9d90d283",
   "metadata": {},
   "source": [
    "For our unsupervised learning task we would like to convert this data into numeric data, that matches the ordering of the possible values in this column.\n",
    "\n",
    "An ordering of the values could be:\n",
    "\n",
    "UNKNOWN<lt100<100to500<500to1000<gt1000\n",
    "\n",
    "(here lt means less than and gt means greater than).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959c939-b8b2-427b-b285-2701e64eada1",
   "metadata": {},
   "source": [
    "To replace the values in the savings column with a numeric value, we can use a Python dictionary.\n",
    "\n",
    "We can create a dictionary using the syntax:\n",
    "\n",
    "```python\n",
    "dict = {\"key1\": \"value1\",\"key2\":\"value2\"} \n",
    "```\n",
    "\n",
    "where the keys are the indexes of the dictionary, and the values are returned for the corresponding key.\n",
    "\n",
    "We can use a dictionary to replace the ordinal data in this way:\n",
    "\n",
    "```python\n",
    "savings_replace = {\"UNKNOWN\":0,\"lt100\":1,\"100to500\":2,\"500to1000\":3,\"gt1000\":4}\n",
    "data['Savings']=data['Savings'].map(savings_replace)\n",
    "```\n",
    "\n",
    "We have used the numbers from 0 to 4 to represent the values, as we will scale them later.\n",
    "\n",
    "The map function is usually used to apply a function to every element in a column, but it can also be used with a dictionary to replace values - any value matching a key is replaced with the associated value in the dictionary.\n",
    "\n",
    "Do this in the cell below, and check that the savings column has been updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bca711-46ed-4722-9751-4975fbb69d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd88487f-c431-4610-9ea7-bad828e93ea3",
   "metadata": {},
   "source": [
    "The checking column also contains ordinal values. Use the same approach as for the savings column to replace them with numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1775c25-2d47-4bbc-9310-985862dc3a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b98492-de61-4692-8444-48b3aa278870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80fe04ea-83c5-4416-accc-33906da590dd",
   "metadata": {},
   "source": [
    "In our data we still have some categorical features, for example the 'Purpose' column. As we did in the previous labs, we can convert these to a 'one-hot' encoding, where a single column is replaced with multiple columns, one for each possible value of the categorical varaible.\n",
    "\n",
    "In pandas the *get_dummies()* function will do this for us:\n",
    "\n",
    "```python\n",
    "data_one_hot=pd.get_dummies(data,drop_first=True)\n",
    "```\n",
    "\n",
    "Do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a953ec5-1626-4f88-a3f4-e68c24df885d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89908b87-dd03-4743-8c57-8d6c2840e287",
   "metadata": {},
   "source": [
    "To see how the one-hot encoding has modified the data, we can look at the 'Housing' column as an example. Look at the original values in the housing column with:\n",
    "\n",
    "```python\n",
    "data['Housing'].unique()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf09e6c-93ae-4d1e-a70d-b7ac88218ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02d5745e-9c5b-4598-865c-fad29cfa7138",
   "metadata": {},
   "source": [
    "Now we can look at the new columns created by *get_dummies()* and compare with the original encoding:\n",
    "\n",
    "```python\n",
    "display(data[['Housing']].head())\n",
    "display(data_one_hot[['Housing_OWN','Housing_RENT']].head())\n",
    "```\n",
    "\n",
    "Notice that one of the values, 'FREE' is missing - this is because 'FREE' is encoded by both 'Housing_OWN' and 'Housing_RENT' being False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b125e1-da4d-4716-9195-8bd2b5a2a829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "569af041-a5b9-47e5-84c5-b92c794e333d",
   "metadata": {},
   "source": [
    "Finally we can drop the target variable from the data (as we are performing unsupervised machine learning), and scale the numeric columns in the data.\n",
    "\n",
    "```python\n",
    "X = data_one_hot.drop(\"Status\",axis=1) # Drop the target feature\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d437683-5124-443b-bf58-78ba16932073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98da6f7f-ff88-47fa-86ec-2d21d2478d50",
   "metadata": {},
   "source": [
    "To scale the data use:\n",
    "\n",
    "```python\n",
    "numeric = ((X.dtypes==np.int64) | (X.dtypes==np.float64 ))\n",
    "numeric_columns = list(numeric.index[numeric==True]) # We want to only scale the numeric columns\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scaled = X.copy() # Make a copy of the data\n",
    "X_scaled[numeric_columns] = min_max_scaler.fit_transform(X_scaled[numeric_columns])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b930f7-8b63-4bcb-aa6b-c97c718e633c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d9f5296-b142-465d-8a1d-f31b36fad5c5",
   "metadata": {},
   "source": [
    "## K means clustering\n",
    "\n",
    "K-means clustering consists of defining clusters such that the total intra-cluster variation (known as total within-cluster variation) is minimised.  There are several k-means algorithms available but the most common (Hartigan and Wong 1979) defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid.\n",
    "\n",
    "$$\\textrm{within\\_cluster\\_variation}=W(C_k)=\\sum_{x_i \\in C_k} (x_i-\\mu_i)^2$$\n",
    "\n",
    "where the $x_i$ are the data points belonging to cluster $C_k$, and $\\mu_i$ is the mean of the data points in cluster $C_k$, corresponding to the centre of the cluster.\n",
    "\n",
    "The total within-cluster variation measures the “goodness of the clustering” and we want this to be as small as possible for our chosen k number of clusters:\n",
    "\n",
    "$$\\textrm{total\\_within}=\\sum_{k=1}^K W(C_k)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1b1f0-f546-48fd-b772-e9237e839fc6",
   "metadata": {},
   "source": [
    "From the lecture, here are the basic k-mean algorithm steps:\n",
    "\n",
    "1.\tYou specify the k number of clusters to be created (e.g. centers=3).\n",
    "2.\tRandomly select k data points (data records) from the dataset as the initial cluster centres.\n",
    "3.\tAssign each data point to their closest centroid, based on the Euclidean distance between the data point and the centroid\n",
    "4.\tFor each of the k clusters, update the cluster centroid by calculating the new mean values of all the data points in the cluster.  The centroid of a kth cluster is record of data, i.e. a vector of length p containing the means of all fields for the data points in the kth cluster; p is the number of fields.\n",
    "5.\tIteratively minimise the total within sum of square, i.e. iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached.\n",
    "\n",
    "You can see that this algorithm works on numeric values and measures the sameness (or difference) using the Euclidean distances between distinct records from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e8e18-5907-4016-b50f-82a3854abc63",
   "metadata": {},
   "source": [
    "### Initial clustering with four clusters\n",
    "\n",
    "To start with try clustering the scaled data using the *scikit-learn* KMeans class, with the number of clusters set to 4.\n",
    "\n",
    "First we can create the python object representing the model, and fit it to the data. *scikit-learn* provides a *fit_predict()* method for some classes that combines fitting and predicting with some data.\n",
    "\n",
    "```python\n",
    "kmeans = KMeans(n_clusters=4,n_init='auto')\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "```\n",
    "\n",
    "Do this below and try looking at the output in the clusters array, giving the cluster assignment of each data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec7c29-258a-4660-b4bb-a9f479be5031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e4adaf2-5e37-467b-a16f-30b2fe430835",
   "metadata": {},
   "source": [
    "To be able to visualise the clustering we need to reduce the number of dimensions in the data. We can do this using PCA and taking the first two principal components in the data to use for plotting in two dimenions.\n",
    "\n",
    "```python\n",
    "pca = PCA()\n",
    "pca_X = pca.fit_transform(X_scaled)\n",
    "plt.scatter(pca_X[:,0],pca_X[:,1],c=clusters)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7403e-ed03-4ea2-b50b-8cb7ff1f0de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e2200da-d1ca-4f62-97f1-6282d23bd38a",
   "metadata": {},
   "source": [
    "The kmeans object representing our data has an attribute *cluster_centers_* that contains the locations of the four cluster centres. We can see their locations by applying the same PCA transformation to them and adding them to the plot.\n",
    "\n",
    "```python\n",
    "# Plot data points\n",
    "pca = PCA()\n",
    "pca_X = pca.fit_transform(X_scaled)\n",
    "plt.scatter(pca_X[:,0],pca_X[:,1],c=clusters)\n",
    "\n",
    "# Add points for the four cluster centres\n",
    "cluster_centres = kmeans.cluster_centers_\n",
    "cs = pca.transform(cluster_centres)\n",
    "plt.scatter(cs[:,0],cs[:,1],c='red',marker='x')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82e6a2-1d6a-4b62-9301-02bef5b1763f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d72d933d-6a13-4215-974d-2072aa6301e4",
   "metadata": {},
   "source": [
    "### Elbow method\n",
    "\n",
    "k-means clustering defines clusters such that the total intra-cluster variation total_within is minimised.  The $\\textrm{total\\_within}$ value measures the “compactness” of the clustering and we want it to be as small as possible.\n",
    "\n",
    "The Elbow method looks at $\\textrm{total\\_within}$ as a function of the number of clusters and you then choose the number of clusters, such that adding another cluster doesn’t improve $\\textrm{total\\_within}$.\n",
    "\n",
    "1. Compute k-means clustering for different values of k, e.g. varying k from 1 to 10 clusters.\n",
    "2. For each k, calculate total_within.\n",
    "3. Plot the curve of total_within  according to the number of clusters k.\n",
    "4. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab8784-b03a-49b4-86e9-78bb84261d65",
   "metadata": {},
   "source": [
    "We can write a loop to calculate the total_within score for a range of values of k (the number of clusters), using the *intertia* attribute of the kmeans model (which returns the total_within score).\n",
    "\n",
    "Use this code as a template to build the loop, filling in the missing parts:\n",
    "\n",
    "```python\n",
    "scores = []\n",
    "ks = list(range(2,16)) # Create a list ranging from 2 to 15\n",
    "for k in ks:\n",
    "    # Fit a model with k clusters and find the intertia\n",
    "    # You can use scores.append() to add to the scores list\n",
    "    ...\n",
    "    ...\n",
    "```\n",
    "\n",
    "Then we can plot the results with:\n",
    "\n",
    "```python\n",
    "plt.plot(ks,scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa022a-e0fe-40e4-92a0-7570356f6952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a2f751f-d435-45db-ac08-b97f8c536d60",
   "metadata": {},
   "source": [
    "### Select k using Average Silhouette Method\n",
    "\n",
    "The average silhouette approach  measures the quality of a clustering so that it determines how well each object lies within its cluster by estimating the average distance between clusters.  A high average silhouette width indicates a good clustering.  The silhouette score is a measure of how close each point in one cluster is to points in the neighbouring clusters.\n",
    "\n",
    "$$\\textrm{silhouette}_i=s(i)=\\frac{b(i)-\\textrm{total\\_within}(i)}{\\textrm{max} (\\textrm{total\\_within}(i),b(i))}$$\n",
    "\n",
    "where $\\textrm{total\\_within}$ is the average distance between data point $i$ and all other records in the same cluster, and $b(i)$ is the smallest average distance between $i$ and all other records in any other cluster.\n",
    "\n",
    "We can say that $\\textrm{total\\_within}(i)$ is a measure of how well the record $i$ is assigned to its cluster, i.e. the smaller the better. $b(i)$ is the average dissimilarity.\n",
    "\n",
    "The average $s(i)$ over all points of a cluster is a measure of how strongly grouped the points in the cluster are.  The $s(i)$ over the entire dataset is a measure of how appropriately the data have been clustered. If there are too many or too few clusters then some of the clusters will typically display much narrower silhouettes than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c946d32-ed00-4f83-8ab5-98f516ebaf8a",
   "metadata": {},
   "source": [
    "To calculate the silhouette score for a clustering, we can use the *silhouette_score()* function. This function takes two arguments, *X*, and *clusters*, where *X* are the data, and *clusters* are the cluster assignments of each data point:\n",
    "\n",
    "```python\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "score = silhouette_score(X_scaled,clusters)\n",
    "```\n",
    "\n",
    "Use this to generate a plot of the silhouette score for different numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e4ca4-c2dd-4023-a16b-12ab006f305e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d445db4f-fceb-4436-a25d-c918aad25eea",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61dd78-cfa8-43f9-8bfb-44b90f27cd41",
   "metadata": {},
   "source": [
    "The key operation in hierarchical agglomerative clustering is to repeatedly combine the two nearest clusters into a larger cluster:\n",
    "\n",
    "1.\tCalculate the distance between every pair of records and store it in a distance matrix.\n",
    "2.\tAllocate every record to its own cluster.\n",
    "3.\tMerging the closest pairs of points based on the distances from the distance matrix and as a result the number of clusters goes down by 1.\n",
    "4.\tRecompute the distance between the new cluster and the old ones and stores in a new distance matrix.\n",
    "5.\tRepeats steps 3 and 4 until all the clusters are merged into one single cluster.\n",
    "\n",
    "In hierarchical clustering, you group the objects into a hierarchy similar to a tree-like diagram which is called a dendrogram. The distance of split or merge (called height) is shown on the y-axis of the dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a0888-9d1b-4b6b-adf1-56844e282a78",
   "metadata": {},
   "source": [
    "In *scikit-learn* you can use the *AgglomerativeClustering* class to create a hierarchical clustering model. The number of clusters is specified by the *n_clusters* argument, e.g.\n",
    "\n",
    "```python\n",
    "hc = AgglomerativeClustering(n_clusters=k)\n",
    "```\n",
    "\n",
    "Try writing Python code to apply hierarchical clustering to the data for a range of numbers of clusters from 2 to 15, and plot the silhouette score for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f85b8e-9223-42a5-bc5a-8f77b6e7fe2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b57810db-5730-43c2-b8f0-5f95404ad492",
   "metadata": {},
   "source": [
    "We can view the tree created by the algorithm using a function from *scipy* to plot the tree.\n",
    "\n",
    "Use this code to plot the tree:\n",
    "\n",
    "```python\n",
    "# Fit the model\n",
    "hc = AgglomerativeClustering(n_clusters=4,compute_distances=True)\n",
    "hc.fit(X_scaled)\n",
    "# Extract the tree\n",
    "children = hc.children_\n",
    "distance = hc.distances_\n",
    "cs = np.arange(2, children.shape[0]+2)\n",
    "linkage_matrix = np.column_stack([children,distance,cs]).astype(float)\n",
    "# Plot the tree - you can change p to change the depth of tree shown\n",
    "_ = dendrogram(linkage_matrix,truncate_mode=\"level\",p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf23b0-f613-44a2-a438-ade52b25c1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28f9437f-666f-471d-a5ef-ca5fabc84b57",
   "metadata": {},
   "source": [
    "Fit an agglomerative clustering model to the scaled data, choosing the number of clusters to create, and use a PCA transform to plot the clusters as we did for the K-means algorithm. Try this for different numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7efc8-80c4-4b59-a936-bdd19baf080b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
