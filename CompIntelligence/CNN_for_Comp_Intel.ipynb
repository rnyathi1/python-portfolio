{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVIHIgLH0CfQ",
        "outputId": "e51e1371-e1e6-4d3a-8bb7-fbae21d81f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'num_cases_per_batch': 10000, 'label_names': ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 'num_vis': 3072}\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "#opening the batches.meta file for info on label names\n",
        "with open('batches.meta', 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "\n",
        "print(meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJa_ZA4_0NAb",
        "outputId": "048f5acb-2afd-4da0-f17e-6d7f0ce48300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images: (50000, 3, 32, 32), Train labels: 50000\n",
            "Test images: (10000, 3, 32, 32), Test labels: 10000\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# Combining all the data into one dataset\n",
        "def load_cifar_batches(batch_folder, num_batches=5):\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    #looping through the folder and unpickling the datasets\n",
        "    for i in range(1, num_batches + 1):\n",
        "        batch_file = os.path.join(batch_folder, f\"data_batch_{i}\")\n",
        "        with open(batch_file, 'rb') as f:\n",
        "            batch = pickle.load(f, encoding = 'bytes')\n",
        "\n",
        "        #extracting and processing data\n",
        "        batch_images = batch[b'data'].reshape(10000, 3, 32, 32).astype('float32') / 255.0\n",
        "        batch_labels = batch[b'labels']\n",
        "\n",
        "        images.append(batch_images)\n",
        "        labels.extend(batch_labels)\n",
        "\n",
        "    #combining all batches into a single dataset & stacking images vertically\n",
        "    images = np.vstack(images)\n",
        "    return images, labels\n",
        "\n",
        "#doing the same thing for testing data\n",
        "def load_test_batch(batch_folder):\n",
        "\n",
        "    batch_file = os.path.join(batch_folder, \"test_batch\")\n",
        "    with open(batch_file, 'rb') as f:\n",
        "        batch = pickle.load(f, encoding = 'bytes')\n",
        "\n",
        "    images = batch[b'data'].reshape(10000, 3, 32, 32).astype('float32') / 255.0\n",
        "    labels = batch[b'labels']\n",
        "    return images, labels\n",
        "\n",
        "#putting functions in action\n",
        "batch_folder = \"/Users/ryannyathi/Documents/CompIntelligence/Cifar_10_data\"  # Path to the folder where the batches are stored\n",
        "train_images, train_labels = load_cifar_batches(batch_folder)\n",
        "test_images, test_labels = load_test_batch(batch_folder)\n",
        "\n",
        "print(f\"Train images: {train_images.shape}, Train labels: {len(train_labels)}\")\n",
        "print(f\"Test images: {test_images.shape}, Test labels: {len(test_labels)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8ja3HbJ0R7K",
        "outputId": "1fd3d7d4-8e9e-4bb9-f484-3c2d7a0864df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader batches: 782, Test loader batches: 157\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#converting to pytorch tensor\n",
        "\n",
        "train_images_tensor = torch.from_numpy(train_images)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n",
        "#doing the same for testing data\n",
        "test_images_tensor = torch.from_numpy(test_images)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "#creating dataloaders\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "#checking the output\n",
        "print(f\"Train loader batches: {len(train_loader)}, Test loader batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DwjnKoH67ZVV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([50000, 10])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#creating cnn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10) #final layer 128 neurons mapped to 10 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8) #flattening the array to create a vector for fc layers\n",
        "        x = torch.relu(self.fc1(x))#passing flattened vector through fc1 with relu activation\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n",
        "\n",
        "model(train_images_tensor).shape #batch size by number of classes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b3xCwySd7x0P"
      },
      "outputs": [],
      "source": [
        "#adding loss for classification and using adam optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr =0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tbQMyUn8DL3",
        "outputId": "bf42348e-5a66-46c0-e0af-f701775627b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.4631\n",
            "Epoch 2, Loss: 0.4631\n",
            "Epoch 3, Loss: 0.4632\n",
            "Epoch 4, Loss: 0.4631\n",
            "Epoch 5, Loss: 0.4633\n",
            "Epoch 6, Loss: 0.4633\n",
            "Epoch 7, Loss: 0.4635\n",
            "Epoch 8, Loss: 0.4634\n",
            "Epoch 9, Loss: 0.4633\n",
            "Epoch 10, Loss: 0.4632\n",
            "Training complete.\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()  # Set model to training mode\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update weights\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n",
        "    print(\"Model saved.\")\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_training_loss(losses):\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY-ZfaJDCcPN",
        "outputId": "3873fe77-d1b7-4b7b-ce81-e2007b550339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 69.58%\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  #setting model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  #disabling gradient for evaluation\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  #getting predicted class\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate_model(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cTU_my_OC2uY"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"baseline_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEYy_JB1EuYf",
        "outputId": "072ba069-cf8a-46e9-c791-603b58191894"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SimpleCNN' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#checking the weights of the gradient based trained model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleCNN\u001b[49m()\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#looping through the layers and print the weights\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SimpleCNN' is not defined"
          ]
        }
      ],
      "source": [
        "#checking the weights of the gradient based trained model\n",
        "model = SimpleCNN()\n",
        "model.load_state_dict(torch.load(\"baseline_model.pth\"))\n",
        "\n",
        "#looping through the layers and print the weights\n",
        "for name, param in model.named_parameters():\n",
        "  print(f\"Layer: {name}\")\n",
        "  print(f\"Weights Shape: {param.shape}\")\n",
        "  print(f\"Weights: {param}\")\n",
        "  print(\"-\" * 20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
