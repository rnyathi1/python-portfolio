{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader batches: 782, Test loader batches: 157\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.init as init\n",
    "\n",
    "#opening the batches.meta file for info on label names\n",
    "with open('batches.meta', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "# Combining all the data into one dataset\n",
    "def load_cifar_batches(batch_folder, num_batches=5):\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    #looping through the folder and unpickling the datasets\n",
    "    for i in range(1, num_batches + 1):\n",
    "        batch_file = os.path.join(batch_folder, f\"data_batch_{i}\")\n",
    "        with open(batch_file, 'rb') as f:\n",
    "            batch = pickle.load(f, encoding = 'bytes')\n",
    "\n",
    "        #extracting and processing data\n",
    "        batch_images = batch[b'data'].reshape(10000, 3, 32, 32).astype('float32') / 255.0\n",
    "        batch_labels = batch[b'labels']\n",
    "\n",
    "        images.append(batch_images)\n",
    "        labels.extend(batch_labels)\n",
    "\n",
    "    #combining all batches into a single dataset & stacking images vertically\n",
    "    images = np.vstack(images)\n",
    "    return images, labels\n",
    "\n",
    "#doing the same thing for testing data\n",
    "def load_test_batch(batch_folder):\n",
    "\n",
    "    batch_file = os.path.join(batch_folder, \"test_batch\")\n",
    "    with open(batch_file, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding = 'bytes')\n",
    "\n",
    "    images = batch[b'data'].reshape(10000, 3, 32, 32).astype('float32') / 255.0\n",
    "    labels = batch[b'labels']\n",
    "    return images, labels\n",
    "\n",
    "#putting functions in action\n",
    "batch_folder = \"/Users/ryannyathi/Documents/CompIntelligence/Cifar_10_data\"  # Path to the folder where the batches are stored\n",
    "train_images, train_labels = load_cifar_batches(batch_folder)\n",
    "test_images, test_labels = load_test_batch(batch_folder)\n",
    "\n",
    "\n",
    "\n",
    "#converting to pytorch tensor\n",
    "\n",
    "train_images_tensor = torch.from_numpy(train_images)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "#doing the same for testing data\n",
    "test_images_tensor = torch.from_numpy(test_images)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "#creating dataloaders\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "#checking the output\n",
    "print(f\"Train loader batches: {len(train_loader)}, Test loader batches: {len(test_loader)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  #setting model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  #disabling gradient for evaluation\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  #getting predicted class\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "   \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/5rjf0wsd6179hwk815vh2rcm0000gn/T/ipykernel_1818/3084219500.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"simple_cnn_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10) #final layer 128 neurons mapped to 10 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8) #flattening the array to create a vector for fc layers\n",
    "        x = torch.relu(self.fc1(x))#passing flattened vector through fc1 with relu activation\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(\"simple_cnn_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the last layers \n",
    "for name, param in model.named_parameters():\n",
    "    if(not 'fc2' in name):\n",
    "        param.requires_grad = False\n",
    "#Randomising last layer weights\n",
    "init.uniform_(model.fc2.weight, a=-0.1, b=0.1)\n",
    "init.uniform_(model.fc2.bias, a=-0.1, b=0.1)\n",
    "\n",
    "# Extracting weights and biases from the last layer\n",
    "last_layer_weights = model.fc2.weight.data.numpy().flatten()\n",
    "last_layer_biases = model.fc2.bias.data.numpy().flatten()\n",
    "\n",
    "# Combining into a single array for optimization\n",
    "params = np.concatenate([last_layer_weights, last_layer_biases])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "initial_accuracy = evaluate_model(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness function \n",
    "def fitness_function(params):\n",
    "    # Split weights and biases\n",
    "    num_weights = model.fc2.weight.numel()\n",
    "    weights = params[:num_weights].reshape(model.fc2.weight.shape)\n",
    "    biases = params[num_weights:]\n",
    "\n",
    "    # Updating the model's last layer weights and biases\n",
    "    model.fc2.weight.data = torch.tensor(weights, dtype=torch.float32)\n",
    "    model.fc2.bias.data = torch.tensor(biases, dtype=torch.float32)\n",
    "\n",
    "    # Evaluating the model's accuracy \n",
    "    \n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    return 100 - accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Differential_Evolution(fitness_function, bounds, population_size, Scaling_Factor, Crossover_Rate, max_iter):\n",
    "    # Initialize population within bounds\n",
    "    n = model.fc2.weight.numel() + model.fc2.bias.numel()\n",
    "\n",
    "    population = np.random.uniform(low=bounds[0], high=bounds[1], size=(population_size, n))\n",
    "    fitness_values = np.array([fitness_function(ind) for ind in population])\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        for i in range(population_size):\n",
    "            # Selecting three random distinct indices\n",
    "            indices = np.random.choice(np.delete(np.arange(population_size), i), 3, replace=False)\n",
    "            a, b, c = population[indices]\n",
    "\n",
    "            # Mutation\n",
    "            mutant_vector = np.clip(a + Scaling_Factor * (b - c), bounds[0], bounds[1])\n",
    "\n",
    "            # Crossover\n",
    "            crossover_mask = np.random.rand(n) <= Crossover_Rate\n",
    "            trial_vector = np.where(crossover_mask, mutant_vector, population[i])\n",
    "\n",
    "    \n",
    "            # Selection\n",
    "            trial_fit = fitness_function(trial_vector)\n",
    "            if trial_fit < fitness_values[i]:\n",
    "                population[i] = trial_vector\n",
    "                fitness_values[i] = trial_fit\n",
    "        # Evaluate accuracy\n",
    "        \n",
    "        accuracy = evaluate_model(model, test_loader)  \n",
    "        print(f\"Accuracy after iteration {iteration + 1}: {accuracy:.2f}%\")\n",
    "\n",
    "    # Return the best solution and its fitness\n",
    "    best_index = fitness_values.argmin()\n",
    "    return population[best_index], fitness_values[best_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy: 7.38%\n",
      "Accuracy after iteration 1: 10.58%\n",
      "Accuracy after iteration 2: 14.86%\n",
      "Accuracy after iteration 3: 10.39%\n",
      "Accuracy after iteration 4: 16.65%\n",
      "Accuracy after iteration 5: 12.47%\n",
      "Accuracy after iteration 6: 15.87%\n",
      "Accuracy after iteration 7: 15.01%\n",
      "Accuracy after iteration 8: 15.00%\n",
      "Accuracy after iteration 9: 11.81%\n",
      "Accuracy after iteration 10: 11.02%\n",
      "Accuracy after iteration 11: 15.29%\n",
      "Accuracy after iteration 12: 15.11%\n",
      "Accuracy after iteration 13: 15.72%\n",
      "Accuracy after iteration 14: 19.44%\n",
      "Accuracy after iteration 15: 19.23%\n",
      "Accuracy after iteration 16: 20.99%\n",
      "Accuracy after iteration 17: 24.41%\n",
      "Accuracy after iteration 18: 21.39%\n",
      "Accuracy after iteration 19: 25.32%\n",
      "Accuracy after iteration 20: 19.89%\n",
      "Accuracy after iteration 21: 23.09%\n",
      "Accuracy after iteration 22: 19.44%\n",
      "Accuracy after iteration 23: 18.66%\n",
      "Accuracy after iteration 24: 20.66%\n",
      "Accuracy after iteration 25: 23.03%\n",
      "Accuracy after iteration 26: 22.94%\n",
      "Accuracy after iteration 27: 16.93%\n",
      "Accuracy after iteration 28: 21.91%\n",
      "Accuracy after iteration 29: 22.93%\n",
      "Accuracy after iteration 30: 23.50%\n",
      "Accuracy after iteration 31: 20.55%\n",
      "Accuracy after iteration 32: 25.22%\n",
      "Accuracy after iteration 33: 24.49%\n",
      "Accuracy after iteration 34: 25.22%\n",
      "Accuracy after iteration 35: 26.82%\n",
      "Accuracy after iteration 36: 21.02%\n",
      "Accuracy after iteration 37: 30.80%\n",
      "Accuracy after iteration 38: 26.07%\n",
      "Accuracy after iteration 39: 28.62%\n",
      "Accuracy after iteration 40: 26.20%\n",
      "Accuracy after iteration 41: 32.59%\n",
      "Accuracy after iteration 42: 31.28%\n",
      "Accuracy after iteration 43: 29.28%\n",
      "Accuracy after iteration 44: 31.80%\n",
      "Accuracy after iteration 45: 31.69%\n",
      "Accuracy after iteration 46: 28.25%\n",
      "Accuracy after iteration 47: 33.07%\n",
      "Accuracy after iteration 48: 29.10%\n",
      "Accuracy after iteration 49: 34.06%\n",
      "Accuracy after iteration 50: 33.99%\n",
      "Accuracy after iteration 51: 32.30%\n",
      "Accuracy after iteration 52: 24.01%\n",
      "Accuracy after iteration 53: 26.79%\n",
      "Accuracy after iteration 54: 32.31%\n",
      "Accuracy after iteration 55: 30.60%\n",
      "Accuracy after iteration 56: 30.51%\n",
      "Accuracy after iteration 57: 28.19%\n",
      "Accuracy after iteration 58: 32.97%\n",
      "Accuracy after iteration 59: 29.62%\n",
      "Accuracy after iteration 60: 31.44%\n",
      "Accuracy after iteration 61: 31.85%\n",
      "Accuracy after iteration 62: 30.36%\n",
      "Accuracy after iteration 63: 34.49%\n",
      "Accuracy after iteration 64: 32.13%\n",
      "Accuracy after iteration 65: 34.54%\n",
      "Accuracy after iteration 66: 34.59%\n",
      "Accuracy after iteration 67: 33.46%\n",
      "Accuracy after iteration 68: 33.66%\n",
      "Accuracy after iteration 69: 34.45%\n",
      "Accuracy after iteration 70: 34.24%\n",
      "Accuracy after iteration 71: 32.79%\n",
      "Accuracy after iteration 72: 35.65%\n",
      "Accuracy after iteration 73: 30.07%\n",
      "Accuracy after iteration 74: 35.74%\n",
      "Accuracy after iteration 75: 33.84%\n",
      "Accuracy after iteration 76: 32.75%\n",
      "Accuracy after iteration 77: 33.68%\n",
      "Accuracy after iteration 78: 35.23%\n",
      "Accuracy after iteration 79: 36.48%\n",
      "Accuracy after iteration 80: 34.55%\n",
      "Accuracy after iteration 81: 38.03%\n",
      "Accuracy after iteration 82: 36.23%\n",
      "Accuracy after iteration 83: 34.80%\n",
      "Accuracy after iteration 84: 36.21%\n",
      "Accuracy after iteration 85: 35.64%\n",
      "Accuracy after iteration 86: 35.89%\n",
      "Accuracy after iteration 87: 36.08%\n",
      "Accuracy after iteration 88: 35.13%\n",
      "Accuracy after iteration 89: 35.16%\n",
      "Accuracy after iteration 90: 33.72%\n",
      "Accuracy after iteration 91: 34.88%\n",
      "Accuracy after iteration 92: 36.28%\n",
      "Accuracy after iteration 93: 37.66%\n",
      "Accuracy after iteration 94: 38.15%\n",
      "Accuracy after iteration 95: 35.89%\n",
      "Accuracy after iteration 96: 36.23%\n",
      "Accuracy after iteration 97: 34.32%\n",
      "Accuracy after iteration 98: 37.56%\n",
      "Accuracy after iteration 99: 37.39%\n",
      "Accuracy after iteration 100: 36.71%\n",
      "Accuracy after iteration 101: 32.58%\n",
      "Accuracy after iteration 102: 36.23%\n",
      "Accuracy after iteration 103: 37.07%\n",
      "Accuracy after iteration 104: 36.48%\n",
      "Accuracy after iteration 105: 37.04%\n",
      "Accuracy after iteration 106: 35.76%\n",
      "Accuracy after iteration 107: 36.31%\n",
      "Accuracy after iteration 108: 35.28%\n",
      "Accuracy after iteration 109: 37.12%\n",
      "Accuracy after iteration 110: 38.29%\n",
      "Accuracy after iteration 111: 39.24%\n",
      "Accuracy after iteration 112: 38.43%\n",
      "Accuracy after iteration 113: 37.82%\n",
      "Accuracy after iteration 114: 38.15%\n",
      "Accuracy after iteration 115: 38.60%\n",
      "Accuracy after iteration 116: 38.28%\n",
      "Accuracy after iteration 117: 33.06%\n",
      "Accuracy after iteration 118: 36.35%\n",
      "Accuracy after iteration 119: 39.36%\n",
      "Accuracy after iteration 120: 39.39%\n",
      "Accuracy after iteration 121: 36.33%\n",
      "Accuracy after iteration 122: 40.13%\n",
      "Accuracy after iteration 123: 35.17%\n",
      "Accuracy after iteration 124: 36.82%\n",
      "Accuracy after iteration 125: 38.91%\n",
      "Accuracy after iteration 126: 35.95%\n",
      "Accuracy after iteration 127: 38.53%\n",
      "Accuracy after iteration 128: 37.99%\n",
      "Accuracy after iteration 129: 37.16%\n",
      "Accuracy after iteration 130: 40.39%\n",
      "Accuracy after iteration 131: 41.81%\n",
      "Accuracy after iteration 132: 38.72%\n",
      "Accuracy after iteration 133: 40.56%\n",
      "Accuracy after iteration 134: 35.77%\n",
      "Accuracy after iteration 135: 38.50%\n",
      "Accuracy after iteration 136: 37.76%\n",
      "Accuracy after iteration 137: 36.62%\n",
      "Accuracy after iteration 138: 36.90%\n",
      "Accuracy after iteration 139: 39.27%\n",
      "Accuracy after iteration 140: 39.57%\n",
      "Accuracy after iteration 141: 39.75%\n",
      "Accuracy after iteration 142: 40.83%\n",
      "Accuracy after iteration 143: 37.91%\n",
      "Accuracy after iteration 144: 37.76%\n",
      "Accuracy after iteration 145: 36.46%\n",
      "Accuracy after iteration 146: 38.91%\n",
      "Accuracy after iteration 147: 38.72%\n",
      "Accuracy after iteration 148: 39.24%\n",
      "Accuracy after iteration 149: 39.47%\n",
      "Accuracy after iteration 150: 38.58%\n",
      "Accuracy after iteration 151: 37.86%\n",
      "Accuracy after iteration 152: 40.82%\n",
      "Accuracy after iteration 153: 42.64%\n",
      "Accuracy after iteration 154: 38.98%\n",
      "Accuracy after iteration 155: 38.69%\n",
      "Accuracy after iteration 156: 41.73%\n",
      "Accuracy after iteration 157: 40.08%\n",
      "Accuracy after iteration 158: 39.02%\n",
      "Accuracy after iteration 159: 36.60%\n",
      "Accuracy after iteration 160: 41.59%\n",
      "Accuracy after iteration 161: 42.42%\n",
      "Accuracy after iteration 162: 40.80%\n",
      "Accuracy after iteration 163: 41.14%\n",
      "Accuracy after iteration 164: 39.38%\n",
      "Accuracy after iteration 165: 39.32%\n",
      "Accuracy after iteration 166: 39.09%\n",
      "Accuracy after iteration 167: 42.98%\n",
      "Accuracy after iteration 168: 41.58%\n",
      "Accuracy after iteration 169: 42.74%\n",
      "Accuracy after iteration 170: 42.61%\n",
      "Accuracy after iteration 171: 39.84%\n",
      "Accuracy after iteration 172: 42.04%\n",
      "Accuracy after iteration 173: 43.49%\n",
      "Accuracy after iteration 174: 39.74%\n",
      "Accuracy after iteration 175: 40.57%\n",
      "Accuracy after iteration 176: 41.87%\n",
      "Accuracy after iteration 177: 42.86%\n",
      "Accuracy after iteration 178: 41.55%\n",
      "Accuracy after iteration 179: 43.16%\n",
      "Accuracy after iteration 180: 44.26%\n",
      "Accuracy after iteration 181: 41.93%\n",
      "Accuracy after iteration 182: 40.70%\n",
      "Accuracy after iteration 183: 41.93%\n",
      "Accuracy after iteration 184: 40.91%\n",
      "Accuracy after iteration 185: 40.54%\n",
      "Accuracy after iteration 186: 42.62%\n",
      "Accuracy after iteration 187: 39.98%\n",
      "Accuracy after iteration 188: 43.45%\n",
      "Accuracy after iteration 189: 45.09%\n",
      "Accuracy after iteration 190: 43.39%\n",
      "Accuracy after iteration 191: 43.93%\n",
      "Accuracy after iteration 192: 38.92%\n",
      "Accuracy after iteration 193: 43.44%\n",
      "Accuracy after iteration 194: 42.53%\n",
      "Accuracy after iteration 195: 44.07%\n",
      "Accuracy after iteration 196: 39.70%\n",
      "Accuracy after iteration 197: 43.83%\n",
      "Accuracy after iteration 198: 43.73%\n",
      "Accuracy after iteration 199: 43.83%\n",
      "Accuracy after iteration 200: 42.35%\n",
      "Accuracy after iteration 201: 44.05%\n",
      "Accuracy after iteration 202: 39.44%\n",
      "Accuracy after iteration 203: 44.28%\n",
      "Accuracy after iteration 204: 44.54%\n",
      "Accuracy after iteration 205: 42.48%\n",
      "Accuracy after iteration 206: 45.41%\n",
      "Accuracy after iteration 207: 44.07%\n",
      "Accuracy after iteration 208: 41.47%\n",
      "Accuracy after iteration 209: 41.85%\n",
      "Accuracy after iteration 210: 46.71%\n",
      "Accuracy after iteration 211: 44.69%\n",
      "Accuracy after iteration 212: 44.20%\n",
      "Accuracy after iteration 213: 44.31%\n",
      "Accuracy after iteration 214: 43.45%\n",
      "Accuracy after iteration 215: 45.28%\n",
      "Accuracy after iteration 216: 44.32%\n",
      "Accuracy after iteration 217: 43.44%\n",
      "Accuracy after iteration 218: 46.42%\n",
      "Accuracy after iteration 219: 46.61%\n",
      "Accuracy after iteration 220: 45.34%\n",
      "Accuracy after iteration 221: 46.63%\n",
      "Accuracy after iteration 222: 47.20%\n",
      "Accuracy after iteration 223: 45.33%\n",
      "Accuracy after iteration 224: 45.09%\n",
      "Accuracy after iteration 225: 45.41%\n",
      "Accuracy after iteration 226: 45.82%\n",
      "Accuracy after iteration 227: 45.26%\n",
      "Accuracy after iteration 228: 45.22%\n",
      "Accuracy after iteration 229: 46.21%\n",
      "Accuracy after iteration 230: 46.73%\n",
      "Accuracy after iteration 231: 45.90%\n",
      "Accuracy after iteration 232: 44.17%\n",
      "Accuracy after iteration 233: 47.52%\n",
      "Accuracy after iteration 234: 45.90%\n",
      "Accuracy after iteration 235: 47.58%\n",
      "Accuracy after iteration 236: 45.70%\n",
      "Accuracy after iteration 237: 44.35%\n",
      "Accuracy after iteration 238: 44.39%\n",
      "Accuracy after iteration 239: 43.48%\n",
      "Accuracy after iteration 240: 47.06%\n",
      "Accuracy after iteration 241: 46.60%\n",
      "Accuracy after iteration 242: 47.69%\n",
      "Accuracy after iteration 243: 45.71%\n",
      "Accuracy after iteration 244: 45.56%\n",
      "Accuracy after iteration 245: 47.00%\n",
      "Accuracy after iteration 246: 47.04%\n",
      "Accuracy after iteration 247: 45.27%\n",
      "Accuracy after iteration 248: 45.00%\n",
      "Accuracy after iteration 249: 45.53%\n",
      "Accuracy after iteration 250: 45.98%\n",
      "Accuracy after iteration 251: 45.73%\n",
      "Accuracy after iteration 252: 45.55%\n",
      "Accuracy after iteration 253: 46.28%\n",
      "Accuracy after iteration 254: 47.13%\n",
      "Accuracy after iteration 255: 46.56%\n",
      "Accuracy after iteration 256: 45.79%\n",
      "Accuracy after iteration 257: 45.70%\n",
      "Accuracy after iteration 258: 47.37%\n",
      "Accuracy after iteration 259: 46.65%\n",
      "Accuracy after iteration 260: 45.39%\n",
      "Accuracy after iteration 261: 46.78%\n",
      "Accuracy after iteration 262: 45.49%\n",
      "Accuracy after iteration 263: 45.24%\n",
      "Accuracy after iteration 264: 48.04%\n",
      "Accuracy after iteration 265: 48.49%\n",
      "Accuracy after iteration 266: 47.48%\n",
      "Accuracy after iteration 267: 48.95%\n",
      "Accuracy after iteration 268: 47.45%\n",
      "Accuracy after iteration 269: 47.89%\n",
      "Accuracy after iteration 270: 45.99%\n",
      "Accuracy after iteration 271: 47.91%\n",
      "Accuracy after iteration 272: 47.86%\n",
      "Accuracy after iteration 273: 47.17%\n",
      "Accuracy after iteration 274: 47.80%\n",
      "Accuracy after iteration 275: 47.96%\n",
      "Accuracy after iteration 276: 45.23%\n",
      "Accuracy after iteration 277: 47.68%\n",
      "Accuracy after iteration 278: 48.29%\n",
      "Accuracy after iteration 279: 48.08%\n",
      "Accuracy after iteration 280: 47.62%\n",
      "Accuracy after iteration 281: 47.48%\n",
      "Accuracy after iteration 282: 45.61%\n",
      "Accuracy after iteration 283: 46.50%\n",
      "Accuracy after iteration 284: 46.55%\n",
      "Accuracy after iteration 285: 45.90%\n",
      "Accuracy after iteration 286: 46.67%\n",
      "Accuracy after iteration 287: 44.99%\n",
      "Accuracy after iteration 288: 46.65%\n",
      "Accuracy after iteration 289: 45.77%\n",
      "Accuracy after iteration 290: 47.93%\n",
      "Accuracy after iteration 291: 48.59%\n",
      "Accuracy after iteration 292: 47.26%\n",
      "Accuracy after iteration 293: 47.49%\n",
      "Accuracy after iteration 294: 46.16%\n",
      "Accuracy after iteration 295: 45.62%\n",
      "Accuracy after iteration 296: 48.01%\n",
      "Accuracy after iteration 297: 46.72%\n",
      "Accuracy after iteration 298: 48.46%\n",
      "Accuracy after iteration 299: 48.98%\n",
      "Accuracy after iteration 300: 49.38%\n",
      "Accuracy after iteration 301: 47.23%\n",
      "Accuracy after iteration 302: 47.02%\n",
      "Accuracy after iteration 303: 45.81%\n",
      "Accuracy after iteration 304: 48.75%\n",
      "Accuracy after iteration 305: 47.38%\n",
      "Accuracy after iteration 306: 46.69%\n",
      "Accuracy after iteration 307: 47.73%\n",
      "Accuracy after iteration 308: 48.18%\n",
      "Accuracy after iteration 309: 46.52%\n",
      "Accuracy after iteration 310: 45.72%\n",
      "Accuracy after iteration 311: 46.04%\n",
      "Accuracy after iteration 312: 45.75%\n",
      "Accuracy after iteration 313: 47.94%\n",
      "Accuracy after iteration 314: 48.76%\n",
      "Accuracy after iteration 315: 48.87%\n",
      "Accuracy after iteration 316: 46.62%\n",
      "Accuracy after iteration 317: 48.80%\n",
      "Accuracy after iteration 318: 44.96%\n",
      "Accuracy after iteration 319: 49.93%\n",
      "Accuracy after iteration 320: 48.57%\n",
      "Accuracy after iteration 321: 48.25%\n",
      "Accuracy after iteration 322: 47.50%\n",
      "Accuracy after iteration 323: 48.49%\n",
      "Accuracy after iteration 324: 48.45%\n",
      "Accuracy after iteration 325: 48.55%\n",
      "Accuracy after iteration 326: 48.27%\n",
      "Accuracy after iteration 327: 47.76%\n",
      "Accuracy after iteration 328: 48.68%\n",
      "Accuracy after iteration 329: 47.32%\n",
      "Accuracy after iteration 330: 47.97%\n",
      "Accuracy after iteration 331: 47.96%\n",
      "Accuracy after iteration 332: 48.33%\n",
      "Accuracy after iteration 333: 47.30%\n",
      "Accuracy after iteration 334: 47.34%\n",
      "Accuracy after iteration 335: 48.42%\n",
      "Accuracy after iteration 336: 49.43%\n",
      "Accuracy after iteration 337: 50.08%\n",
      "Accuracy after iteration 338: 47.45%\n",
      "Accuracy after iteration 339: 47.51%\n",
      "Accuracy after iteration 340: 47.76%\n",
      "Accuracy after iteration 341: 48.67%\n",
      "Accuracy after iteration 342: 49.04%\n",
      "Accuracy after iteration 343: 48.19%\n",
      "Accuracy after iteration 344: 48.32%\n",
      "Accuracy after iteration 345: 49.83%\n",
      "Accuracy after iteration 346: 47.93%\n",
      "Accuracy after iteration 347: 49.06%\n",
      "Accuracy after iteration 348: 47.66%\n",
      "Accuracy after iteration 349: 49.65%\n",
      "Accuracy after iteration 350: 49.03%\n",
      "Accuracy after iteration 351: 48.90%\n",
      "Accuracy after iteration 352: 47.21%\n",
      "Accuracy after iteration 353: 47.55%\n",
      "Accuracy after iteration 354: 48.79%\n",
      "Accuracy after iteration 355: 49.27%\n",
      "Accuracy after iteration 356: 49.61%\n",
      "Accuracy after iteration 357: 47.85%\n",
      "Accuracy after iteration 358: 48.33%\n",
      "Accuracy after iteration 359: 46.35%\n",
      "Accuracy after iteration 360: 48.95%\n",
      "Accuracy after iteration 361: 48.97%\n",
      "Accuracy after iteration 362: 49.46%\n",
      "Accuracy after iteration 363: 49.23%\n",
      "Accuracy after iteration 364: 47.17%\n",
      "Accuracy after iteration 365: 48.05%\n",
      "Accuracy after iteration 366: 47.57%\n",
      "Accuracy after iteration 367: 48.00%\n",
      "Accuracy after iteration 368: 50.77%\n",
      "Accuracy after iteration 369: 49.85%\n",
      "Accuracy after iteration 370: 48.04%\n",
      "Accuracy after iteration 371: 49.74%\n",
      "Accuracy after iteration 372: 48.71%\n",
      "Accuracy after iteration 373: 47.53%\n",
      "Accuracy after iteration 374: 47.07%\n",
      "Accuracy after iteration 375: 48.65%\n",
      "Accuracy after iteration 376: 47.17%\n",
      "Accuracy after iteration 377: 47.66%\n",
      "Accuracy after iteration 378: 48.06%\n",
      "Accuracy after iteration 379: 49.54%\n",
      "Accuracy after iteration 380: 49.88%\n",
      "Accuracy after iteration 381: 49.12%\n",
      "Accuracy after iteration 382: 49.94%\n",
      "Accuracy after iteration 383: 49.88%\n",
      "Accuracy after iteration 384: 49.51%\n",
      "Accuracy after iteration 385: 49.94%\n",
      "Accuracy after iteration 386: 50.46%\n",
      "Accuracy after iteration 387: 49.20%\n",
      "Accuracy after iteration 388: 48.31%\n",
      "Accuracy after iteration 389: 48.38%\n",
      "Accuracy after iteration 390: 48.10%\n",
      "Accuracy after iteration 391: 50.34%\n",
      "Accuracy after iteration 392: 49.20%\n",
      "Accuracy after iteration 393: 49.62%\n",
      "Accuracy after iteration 394: 50.13%\n",
      "Accuracy after iteration 395: 49.63%\n",
      "Accuracy after iteration 396: 50.90%\n",
      "Accuracy after iteration 397: 49.71%\n",
      "Accuracy after iteration 398: 49.42%\n",
      "Accuracy after iteration 399: 49.57%\n",
      "Accuracy after iteration 400: 48.79%\n",
      "Accuracy after iteration 401: 48.70%\n",
      "Accuracy after iteration 402: 49.80%\n",
      "Accuracy after iteration 403: 47.87%\n",
      "Accuracy after iteration 404: 50.07%\n",
      "Accuracy after iteration 405: 49.06%\n",
      "Accuracy after iteration 406: 49.87%\n",
      "Accuracy after iteration 407: 49.24%\n",
      "Accuracy after iteration 408: 46.89%\n",
      "Accuracy after iteration 409: 50.29%\n",
      "Accuracy after iteration 410: 48.10%\n",
      "Accuracy after iteration 411: 48.90%\n",
      "Accuracy after iteration 412: 49.68%\n",
      "Accuracy after iteration 413: 47.73%\n",
      "Accuracy after iteration 414: 49.67%\n",
      "Accuracy after iteration 415: 48.83%\n",
      "Accuracy after iteration 416: 49.08%\n",
      "Accuracy after iteration 417: 50.00%\n",
      "Accuracy after iteration 418: 50.24%\n",
      "Accuracy after iteration 419: 48.23%\n",
      "Accuracy after iteration 420: 49.94%\n",
      "Accuracy after iteration 421: 48.74%\n",
      "Accuracy after iteration 422: 50.14%\n",
      "Accuracy after iteration 423: 49.69%\n",
      "Accuracy after iteration 424: 50.53%\n",
      "Accuracy after iteration 425: 50.49%\n",
      "Accuracy after iteration 426: 50.33%\n",
      "Accuracy after iteration 427: 50.69%\n",
      "Accuracy after iteration 428: 51.14%\n",
      "Accuracy after iteration 429: 46.08%\n",
      "Accuracy after iteration 430: 51.00%\n",
      "Accuracy after iteration 431: 45.40%\n",
      "Accuracy after iteration 432: 50.62%\n",
      "Accuracy after iteration 433: 49.97%\n",
      "Accuracy after iteration 434: 47.99%\n",
      "Accuracy after iteration 435: 49.31%\n",
      "Accuracy after iteration 436: 49.45%\n",
      "Accuracy after iteration 437: 49.97%\n",
      "Accuracy after iteration 438: 47.46%\n",
      "Accuracy after iteration 439: 49.51%\n",
      "Accuracy after iteration 440: 50.90%\n",
      "Accuracy after iteration 441: 47.85%\n",
      "Accuracy after iteration 442: 49.91%\n",
      "Accuracy after iteration 443: 49.55%\n",
      "Accuracy after iteration 444: 49.83%\n",
      "Accuracy after iteration 445: 47.25%\n",
      "Accuracy after iteration 446: 49.39%\n",
      "Accuracy after iteration 447: 49.72%\n",
      "Accuracy after iteration 448: 49.14%\n",
      "Accuracy after iteration 449: 51.09%\n",
      "Accuracy after iteration 450: 49.66%\n",
      "Accuracy after iteration 451: 49.63%\n",
      "Accuracy after iteration 452: 49.79%\n",
      "Accuracy after iteration 453: 49.45%\n",
      "Accuracy after iteration 454: 49.68%\n",
      "Accuracy after iteration 455: 51.07%\n",
      "Accuracy after iteration 456: 51.44%\n",
      "Accuracy after iteration 457: 49.62%\n",
      "Accuracy after iteration 458: 50.80%\n",
      "Accuracy after iteration 459: 49.81%\n",
      "Accuracy after iteration 460: 51.55%\n",
      "Accuracy after iteration 461: 49.52%\n",
      "Accuracy after iteration 462: 51.40%\n",
      "Accuracy after iteration 463: 50.25%\n",
      "Accuracy after iteration 464: 51.17%\n",
      "Accuracy after iteration 465: 50.77%\n",
      "Accuracy after iteration 466: 49.74%\n",
      "Accuracy after iteration 467: 50.06%\n",
      "Accuracy after iteration 468: 50.93%\n",
      "Accuracy after iteration 469: 51.21%\n",
      "Accuracy after iteration 470: 49.32%\n",
      "Accuracy after iteration 471: 50.63%\n",
      "Accuracy after iteration 472: 50.78%\n",
      "Accuracy after iteration 473: 50.75%\n",
      "Accuracy after iteration 474: 48.84%\n",
      "Accuracy after iteration 475: 50.17%\n",
      "Accuracy after iteration 476: 50.79%\n",
      "Accuracy after iteration 477: 51.41%\n",
      "Accuracy after iteration 478: 49.54%\n",
      "Accuracy after iteration 479: 50.50%\n",
      "Accuracy after iteration 480: 51.54%\n",
      "Accuracy after iteration 481: 49.99%\n",
      "Accuracy after iteration 482: 50.20%\n",
      "Accuracy after iteration 483: 49.56%\n",
      "Accuracy after iteration 484: 51.39%\n",
      "Accuracy after iteration 485: 51.52%\n",
      "Accuracy after iteration 486: 49.93%\n",
      "Accuracy after iteration 487: 50.66%\n",
      "Accuracy after iteration 488: 51.45%\n",
      "Accuracy after iteration 489: 50.72%\n",
      "Accuracy after iteration 490: 50.23%\n",
      "Accuracy after iteration 491: 50.73%\n",
      "Accuracy after iteration 492: 49.92%\n",
      "Accuracy after iteration 493: 49.40%\n",
      "Accuracy after iteration 494: 50.99%\n",
      "Accuracy after iteration 495: 51.66%\n",
      "Accuracy after iteration 496: 49.60%\n",
      "Accuracy after iteration 497: 49.12%\n",
      "Accuracy after iteration 498: 50.92%\n",
      "Accuracy after iteration 499: 48.98%\n",
      "Accuracy after iteration 500: 48.16%\n",
      "Accuracy after iteration 501: 50.18%\n",
      "Accuracy after iteration 502: 50.93%\n",
      "Accuracy after iteration 503: 51.16%\n",
      "Accuracy after iteration 504: 51.38%\n",
      "Accuracy after iteration 505: 50.05%\n",
      "Accuracy after iteration 506: 48.09%\n",
      "Accuracy after iteration 507: 49.99%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitial Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDifferential_Evolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitness_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfitness_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mScaling_Factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mCrossover_Rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mDifferential_Evolution\u001b[0;34m(fitness_function, bounds, population_size, Scaling_Factor, Crossover_Rate, max_iter)\u001b[0m\n\u001b[1;32m     19\u001b[0m trial_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(crossover_mask, mutant_vector, population[i])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Selection\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m trial_fit \u001b[38;5;241m=\u001b[39m \u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trial_fit \u001b[38;5;241m<\u001b[39m fitness_values[i]:\n\u001b[1;32m     25\u001b[0m     population[i] \u001b[38;5;241m=\u001b[39m trial_vector\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mfitness_function\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(biases, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluating the model's accuracy \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m-\u001b[39m accuracy\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m#disabling gradient for evaluation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m----> 7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m#getting predicted class\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m) \u001b[38;5;66;03m#flattening the array to create a vector for fc layers\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Initial Accuracy: {initial_accuracy:.2f}%')\n",
    "result = Differential_Evolution(fitness_function=fitness_function,bounds=(-1, 1),population_size=10, Scaling_Factor=0.9,Crossover_Rate=0.7,max_iter=1000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
